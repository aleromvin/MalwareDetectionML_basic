# Progress diary

## 1. Preliminary work: finding the dataset and other resources 

The first step in this project, and an often overlooked one, was to find everything that I needed and gathering it in an organized and convenient way (even before that, I had to decide what it was exactly that the project was gonna be about, but I think documenting *that* part of the process would be going too far into topics more suited for psychology and even psychiatry...). To get straight to the point: among the most crucial things that any data scientist needs to get started with any project is, of course, the dataset, so I started there. A great resource for finding useful datasets is Kaggle (I am not promoted by Google and have many reservations about their current role in the current Internet ecosystem, but credit has to be given where it is due), so I looked up "Malware Classification" over there and immediately found a nice looking [dataset](https://www.kaggle.com/datasets/saurabhshahane/classification-of-malwares).

The dataset seemed good enough when I started exploring it except for one detail: I didn't understand what most of the columns meant, or how the data that was present in them was harvested in the first place! So I looked around to try to find any reference to this and, indeed, I saw that in the description of the dataset an article was being cited. This article is the one that I have already cited by Ajit Kumar and friends. I also found that there was an associated [github repository](https://github.com/urwithajit9/ClaMP), which has been quite useful as it contains the Python code that was used for both extracting the raw features from PE headers and subsequently constructing the derived or "integrated" features (a little problem with this code is that it is written in an [older version of python](https://github.com/urwithajit9/ClaMP/issues/5), but I am more than happy to rewrite it, or at least the parts of it that I will make use of). This repository also contains the final version of the first (and as of the moment of writing this, only) dataset that is used in this project. 

## 2. Exploration of the data and beginning of the development of the main shiny document

First I just played around with the data a bit to understand its general structure, nothing noteworthy. Then I started developing the data visualization "infrastructure". In all cases, data is displayed separated by classes (i.e., "benign" and "malware"). To allow for interactive visualization of the data, several functions where written with the intent of eventually being integrated into a shiny app or shiny document (I personally like shiny documents better â€” they look more human-readable and structured to me). 

## 3. Constructing the main shiny document

By this point, I had to plan the general structure of the main shiny document. I came up with this:

- One first tab explaining general stuff like this, perhaps in less detail

- One second tab with the data visualization and manipulation part

- One third tab with the "uploaded malware analysis" part, and perhaps another tab with information about how each of the models perform. 

I personally find it much more convenient and natural to write Rmarkdown documents with embedded R shiny code than to write pure shiny apps, so that is what I did.

## 4. Feature engineering and machine learning models

This part is more or less self-documented in the "model1.R" script, but, basically, I built three models: decision tree, random forest and multilayer perceptron. For the feature engineering part specially, I made extensive use of the data exploration tools built in step #3.

## 5. Evaluating the models 

This part is documented in the app itself.

## 6. Building the interactive analysis part of the app

This part was probably the most challenging for me, as I am not officially trained as a software developer, but I think the learning process was also quite rewarding and it gave me new insights into how to integrate scripts written in different programming languages. Without going into too much detail, whenever a file is uploaded and the button for analyzing is "clicked", a system call to run a python script, with the path to a tempfile which stores a replica of the uploaded file as an aditional argument, is made. When the script ends processing the file for extracting its features, it prints this output to `stout` and R catches it for further processing.

## 7. Writing the documentation

Last but not least, I had to write all of the documentation for this project (including this file right now). Actually, I didn't write *all* the documentation after having finished the project; rather, I updated it as I kept adding new stuff.

## 8? Polishing the project in the future

Although the project is pretty much as done as I wanted it, I still have to add some things to it in the future. Most importantly, I have to compile a custom dataset of my own; I just don't have time or energy to do this right now, but it shouldn't be too difficult. Beside that, I also may want to polish some of the custom CSS in the app, and, of course, we always have to look out for possible bugs.

### 8.1. Adding multithreading capabilities

If the app is hosted on a remote server for anyone to access, it should have multithreading capabilities to handle several user connections simultaneously. I am currently working on this.

### 8.2. Better neural network models

The current neural network is pretty lame, this is understandable because I was running low on time and had to turn in my project for grading (I got a 9.2/10 by the way!). Anyways, from my own judgement of the current situation, I find that the best and most versatile language for training neural networks is Python, so I may implement some python models in the future.
