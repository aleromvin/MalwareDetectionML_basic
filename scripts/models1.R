# Machine learning models for malware detection
# Alexander Romero Vinogradov, <aleromvin@gmail.com>
#
# In this script, I construct the models that will be used later for malware classification.
# I will build four models:
# - Decision tree
# - Random forest
# - Multilayer perceptron
#
# Decision tree will give us the most simple model, and the easiest one to interpret.
# Random forest is harder to interpret, but may yield better results.
# Multilayer perceptron yields the hardest to interpret model. In some situations it will work better than DT and RF, in others it won't â€“ we'll have to see.

# Random seed

set.seed(334485531)

# Libraries

library(tidyverse)
library(tidymodels)
library(farff)
library(corrplot)

# Loading data:

readARFF("datos/ClaMP_Integrated-5184.arff") %>% 
  select(-packer) -> malware

malware$class <- recode(malware$class, `0` = "Benign", `1` = "Malware")

################################
# DT and RF models

## Feature engineering and pre-processing

malware_split <- initial_split(malware, prop = 0.75, strata = class)

malware_training <- malware_split %>% training()
malware_testing <- malware_split %>% testing()

# complete.cases(malware) %>% all() # No lost values -> No need for imputing

# # First we look for correlated numerical variables
# 
# (malware_training %>% 
#   select_if(is.numeric) %>% 
#   cor() -> matcor)
# 
# matcor %>% corrplot()
# 
# # Mostly not too high correlation between variables 
# # However we do see some exceptions
# 
# # Before removing variables with high correlation, though, we will apply transformations for lowering the skewness of a subset of the numeric variables. For visualization purposes, let's apply this transformation to the data directly:
# 
# (malware_training %>% 
#     mutate(
#       across(
#         tidyselect::starts_with("e_") | c(SizeOfCode, SizeOfInitializedData,
#         SizeOfUninitializedData, AddressOfEntryPoint, BaseOfCode,
#         BaseOfData, CheckSum, SizeOfStackReserve, SizeOfStackCommit,
#         SizeOfHeapReserve, SizeOfHeapCommit, sus_sections, filesize), asinh)
#       ) %>%
#     select_if(is.numeric) %>% 
#     cor() -> matcor2
# )
# 
# corrplot(matcor2)

# Instead of using some fancy transformation like Box-Cox, we're just going to apply an asinh transformation to valuables that I consider to be "very skewed"

malware_recipe <- recipe(class ~ .,
                         data = malware_training) %>%
  
  # step_novel(tidyselect::starts_with("FH_char"),tidyselect::starts_with("OH_DLLchar"),
  #            CreationYear, ImageBase, SectionAlignment, FileAlignment,
  #            SizeOfImage, SizeOfHeaders, LoaderFlags, fileinfo) %>% 
  
  step_other(tidyselect::starts_with("Major"), tidyselect::starts_with("Minor"),
             Subsystem, packer_type) %>% 
  
  step_hyperbolic(tidyselect::starts_with("e_"), SizeOfCode, SizeOfInitializedData,
                  SizeOfUninitializedData, AddressOfEntryPoint, BaseOfCode,
                  BaseOfData, CheckSum, SizeOfStackReserve, SizeOfStackCommit,
                  SizeOfHeapReserve, SizeOfHeapCommit, sus_sections, filesize,
                  func = "sinh", inverse = T) %>%
  
  step_corr(all_numeric(), threshold = 0.9) %>% 
  
  step_normalize(all_numeric()) %>% 
  
  step_dummy(all_factor(), -all_outcomes())

malware_recipe %>%
  prep(training = malware_training) %>% 
  bake(new_data = NULL)

# Notice that the final result is equivalent to what we would have gotten if we had used the "packer" variable instead of "packer_type", 'cause it just dumps all packers into the "other" category

#######################
# Decision Tree Model #
#######################

# 1. 

# - Cross validation for choosing the right model + evaluating the selected model
#  

malware_metrics <- metric_set(roc_auc, sens, spec, accuracy)

# A first set of folds for tuning with cv. We can also use these same folds to evaluate the chosen model

malware_folds1 <- vfold_cv(malware_training, v = 10, strata = class)

dt_tune_model <- decision_tree(cost_complexity = tune(),
                               tree_depth = tune(),
                               min_n = tune()) %>% 
  set_engine('rpart') %>%
  set_mode('classification')

# Workflows somehow make everything more convenient

malware_tune_dt_workflow <- workflow() %>% 
  add_model(dt_tune_model) %>% 
  add_recipe(malware_recipe,
             blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE))

dt_grid <- grid_random(extract_parameter_set_dials(dt_tune_model), size = 8) # 8 is my favorite number, not quite as big as 9 or 10, but quite bigger than 6 and 7

dt_tuning <- malware_tune_dt_workflow %>% 
  tune_grid(resamples = malware_folds1,
            grid = dt_grid,
            metrics = malware_metrics) # Doesn't take too long

best_dt_model <- dt_tuning %>% 
  select_best(metric = 'spec')

best_dt_model

malware_final_dt_workflow <- malware_tune_dt_workflow %>% 
  finalize_workflow(best_dt_model)

# Now we build and evaluate the model

# We evaluate in two ways:
# 1. Cross validation
# 2. With the test split

malware_final_dt_fit <- malware_final_dt_workflow %>% 
  last_fit(split = malware_split, metrics = malware_metrics)

malware_final_dt_predictions <- malware_final_dt_fit %>%
  collect_predictions()

##################################
# Random forest: same recipe

malware_folds2 <- vfold_cv(malware_training, v = 10, strata = class)

rf_tune_model <- rand_forest(mtry = tune(),
                             trees = tune(),
                             min_n = tune()) %>%
  set_engine(engine = 'ranger', importance = "impurity") %>% 
  set_mode('classification')

# Workflows somehow make everything more convenient

malware_tune_rf_workflow <- workflow() %>% 
  add_model(rf_tune_model) %>% 
  add_recipe(malware_recipe,
             blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE))

rf_grid <- grid_random(mtry() %>% finalize(malware_training), min_n(), size = 5)
rf_grid$trees <- c(50,100,200,400,800) # I made a smaller grid 'cause I didn't want to crash my PC

rf_tuning <- malware_tune_rf_workflow %>% 
  tune_grid(resamples = malware_folds2,
            grid = rf_grid,
            metrics = malware_metrics) # Surprisingly doesn't take too long either

# We're gonna use specificity as the metric as our main objective is to detect viruses
# Which in this case are the "negative" class (a bit confusing I know, perhaps we should have re-encoded the classes)

best_rf_model <- rf_tuning %>% 
  select_best(metric = 'spec')

malware_final_rf_workflow <- malware_tune_rf_workflow %>% 
  finalize_workflow(best_rf_model)

# Now we build and evaluate the model

# We evaluate in three ways:
# 1. Cross validation
# 2. With the test split
# 3. Out of bag

malware_final_rf_fit <- malware_final_rf_workflow %>% 
  last_fit(split = malware_split, metrics = malware_metrics)

malware_final_rf_predictions <- malware_final_rf_fit %>%
  collect_predictions()


######################################
# Final model: multilayer perceptron

# We change the recipe a bit, one-hot-encoding for factor works better in this case (http://www.bzst.com/2014/03/the-use-of-dummy-variables-in.html)

malware_recipe2 <- recipe(class ~ .,
                         data = malware_training) %>%
  
  step_other(tidyselect::starts_with("Major"), tidyselect::starts_with("Minor"),
             Subsystem, packer_type) %>% 
  
  step_hyperbolic(tidyselect::starts_with("e_"), SizeOfCode, SizeOfInitializedData,
                  SizeOfUninitializedData, AddressOfEntryPoint, BaseOfCode,
                  BaseOfData, CheckSum, SizeOfStackReserve, SizeOfStackCommit,
                  SizeOfHeapReserve, SizeOfHeapCommit, sus_sections, filesize,
                  func = "sinh", inverse = T) %>%
  
  step_corr(all_numeric(), threshold = 0.9) %>% 
  
  step_normalize(all_numeric()) %>% 
  
  step_dummy(all_factor(), -all_outcomes(), one_hot = T)

# Folds for cross-validation

malware_folds3 <- vfold_cv(malware_training, v = 10, strata = class)

mlp_tune_model <- mlp(hidden_units = tune(), penalty = tune(), epochs = 50) %>% 
  set_engine('nnet', MaxNWts = 2000) %>%
  set_mode('classification')

# Workflows somehow make everything more convenient

malware_tune_mlp_workflow <- workflow() %>% 
  add_model(mlp_tune_model) %>% 
  add_recipe(malware_recipe2,
             blueprint = hardhat::default_recipe_blueprint(allow_novel_levels = TRUE))

mlp_grid <- grid_random(extract_parameter_set_dials(mlp_tune_model), size = 8)

mlp_tuning <- malware_tune_mlp_workflow %>% 
  tune_grid(resamples = malware_folds3,
            grid = mlp_grid,
            metrics = malware_metrics)

best_mlp_model <- mlp_tuning %>% 
  select_best(metric = 'spec')

malware_final_mlp_workflow <- malware_tune_mlp_workflow %>% 
  finalize_workflow(best_mlp_model)

# Now we build and evaluate the model

# We evaluate in two ways:
# 1. Cross validation
# 2. With the test split

malware_final_mlp_fit <- malware_final_mlp_workflow %>% 
  last_fit(split = malware_split, metrics = malware_metrics)

malware_final_mlp_predictions <- malware_final_mlp_fit %>%
  collect_predictions()

# Finally I save the workspace to avoid having to repeat all the calculations

save.image(file = "./modelos/models1.RData")
